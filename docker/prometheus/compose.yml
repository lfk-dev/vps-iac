name: prometheus
services:
  prometheus:
    image: prom/prometheus:v3.9.1
    restart: unless-stopped
    container_name: prometheus
    # TODO: seed the file into the volue during deployment
    # Current solution needs a local copy of the prometheus.yml file on VPS, since you can't bind mount a file during
    # github workflow deployment.
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - config:/etc/prometheus/:ro
    networks:
      - monitoring # seperate monitoring network
      - proxy # traefik discovery network
    command:
      - "--storage.tsdb.retention.size=10GB" # max size of TSDB
      - "--config.file=/etc/prometheus/prometheus.yml"
      # rule path is defined in the config
    labels:
      - "traefik.enable=true" # include in traefik discovery
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${VPS_DOMAIN}`)"
      - "traefik.http.routers.prometheus.entrypoints=websecure"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.routers.prometheus.tls.certresolver=myresolver"
      - "traefik.http.middlewares.prometheus-auth.basicauth.users=${TRAEFIK_DASHBOARD_HASH}"
      - "traefik.http.routers.prometheus.middlewares=prometheus-auth@docker"
    depends_on:
      config-loader:
        condition: service_completed_successfully
  # Exporter mounts the node's directories that store OS information. Containerized exporter monitors those dirs and exposes API/metrics
  # The prometheus container scrapes the metrics from exporter container
  node-exporter:
    image: prom/node-exporter:latest # TODO: find specific version
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc" # exporter commands
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)" # collector command
      # node exporter monitors the mounted node's system dirs
      # the last command specifies to exclude the container dirs (so the exporter does not monitor itself by mistake)
    expose:
      - 9100 # not "ports", but "expose", because it's internal to monitoring network only
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:v0.30.1
    container_name: alertmanager
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    volumes:
      - config:/etc/prometheus/:ro
      - alertmanager-data:/alertmanager
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alertmanager.${VPS_DOMAIN}`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.routers.alertmanager.tls.certresolver=myresolver"
      - "traefik.http.middlewares.alertmanager-auth.basicauth.users=${TRAEFIK_DASHBOARD_HASH}"
      - "traefik.http.routers.alertmanager.middlewares=alertmanager-auth@docker"
    command:
      - "--config.file=/etc/prometheus/alertmanager/alertmanager.yml"

  config-loader:
    image: alpine
    volumes:
      - config:/config
      - /etc/srv/prometheus/:/source/:ro
    command:
      [
        "sh",
        "-c",
        "rm -rf /config/* && cp -r /source/* /config/ && chmod -R a+rX /config/",
      ]
  # NOTE: This is a workaround for unpredictable ownership patterns inside containers
  # Bind mounts use 1:1 mapping of ownership and permissions, so that would
  # require to check every image to see what UID:GID is required.
  # This solves the issues by making the files readable and directories
  # readable and executable for all.

volumes:
  data:
  config:
  alertmanager-data:

networks:
  monitoring:
    name: monitoring
    driver: bridge
  proxy:
    name: proxy
    external: true
